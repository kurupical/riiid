{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "loss - 0.3656: 100%|██████████| 39/39 [00:05<00:00,  6.99it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0 train_loss - 0.39 acc - 0.547 auc - 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.3530: 100%|██████████| 39/39 [00:05<00:00,  7.17it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 1 train_loss - 0.36 acc - 0.576 auc - 0.6114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.3060: 100%|██████████| 39/39 [00:05<00:00,  7.02it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 2 train_loss - 0.33 acc - 0.608 auc - 0.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2840: 100%|██████████| 39/39 [00:05<00:00,  7.02it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 3 train_loss - 0.29 acc - 0.629 auc - 0.6825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2623: 100%|██████████| 39/39 [00:05<00:00,  7.07it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 4 train_loss - 0.27 acc - 0.643 auc - 0.7005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2646: 100%|██████████| 39/39 [00:05<00:00,  6.88it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 5 train_loss - 0.26 acc - 0.650 auc - 0.7085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2562: 100%|██████████| 39/39 [00:05<00:00,  7.10it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 6 train_loss - 0.25 acc - 0.652 auc - 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2593: 100%|██████████| 39/39 [00:05<00:00,  6.90it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 7 train_loss - 0.24 acc - 0.653 auc - 0.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2358: 100%|██████████| 39/39 [00:05<00:00,  6.95it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 8 train_loss - 0.23 acc - 0.650 auc - 0.7076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2130: 100%|██████████| 39/39 [00:05<00:00,  6.87it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 9 train_loss - 0.22 acc - 0.651 auc - 0.7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2136: 100%|██████████| 39/39 [00:05<00:00,  6.88it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 10 train_loss - 0.21 acc - 0.652 auc - 0.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.2070: 100%|██████████| 39/39 [00:05<00:00,  6.95it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 11 train_loss - 0.20 acc - 0.650 auc - 0.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1742: 100%|██████████| 39/39 [00:05<00:00,  6.96it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 12 train_loss - 0.19 acc - 0.653 auc - 0.7068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1560: 100%|██████████| 39/39 [00:05<00:00,  6.86it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 13 train_loss - 0.18 acc - 0.655 auc - 0.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1718: 100%|██████████| 39/39 [00:05<00:00,  7.00it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 14 train_loss - 0.17 acc - 0.657 auc - 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1686: 100%|██████████| 39/39 [00:05<00:00,  7.02it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 15 train_loss - 0.16 acc - 0.659 auc - 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1589: 100%|██████████| 39/39 [00:05<00:00,  6.99it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 16 train_loss - 0.16 acc - 0.661 auc - 0.7152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1463: 100%|██████████| 39/39 [00:05<00:00,  6.70it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 17 train_loss - 0.15 acc - 0.661 auc - 0.7164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1420: 100%|██████████| 39/39 [00:05<00:00,  6.77it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 18 train_loss - 0.14 acc - 0.659 auc - 0.7184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss - 0.1269: 100%|██████████| 39/39 [00:05<00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 19 train_loss - 0.13 acc - 0.665 auc - 0.7211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class SAKTDataset(Dataset):\n",
    "    def __init__(self, group, n_skill, max_seq=100, is_test=False):\n",
    "        super(SAKTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.n_skill = n_skill\n",
    "        self.samples = group\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa, is_val = group[user_id]\n",
    "            if not is_test:\n",
    "                self.user_ids.append([user_id, -1])\n",
    "            else:\n",
    "                for i in range(len(q)):\n",
    "                    if is_val[i]:\n",
    "                        self.user_ids.append([user_id, i+1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index][0]\n",
    "        end = self.user_ids[index][1]\n",
    "        q_, qa_, _ = self.samples[user_id]\n",
    "\n",
    "        if not self.is_test:\n",
    "            seq_len = len(q_)\n",
    "        else:\n",
    "            start = np.max([0, end - self.max_seq])\n",
    "            q_ = q_[start:end]\n",
    "            qa_ = qa_[start:end]\n",
    "            seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        if seq_len >= self.max_seq:\n",
    "            q[:] = q_[-self.max_seq:]\n",
    "            qa[:] = qa_[-self.max_seq:]\n",
    "        else:\n",
    "            q[-seq_len:] = q_\n",
    "            qa[-seq_len:] = qa_\n",
    "\n",
    "        target_id = q[1:]\n",
    "        label = qa[1:]\n",
    "\n",
    "        x = np.zeros(self.max_seq - 1, dtype=int)\n",
    "        x = q[:-1].copy()\n",
    "        x += (qa[:-1] == 1) * self.n_skill\n",
    "\n",
    "        return x, target_id, label\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "class SAKTModel(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=100, embed_dim=128):\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.n_skill = n_skill\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(2 * n_skill + 1, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq - 1, embed_dim)\n",
    "        self.e_embedding = nn.Embedding(n_skill + 1, embed_dim)\n",
    "\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, question_ids):\n",
    "        device = x.device\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "\n",
    "        pos_x = self.pos_embedding(pos_id)\n",
    "        x = x + pos_x\n",
    "\n",
    "        e = self.e_embedding(question_ids)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        e = e.permute(1, 0, 2)\n",
    "        att_mask = future_mask(x.size(0)).to(device)\n",
    "        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n",
    "        att_output = self.layer_normal(att_output + e)\n",
    "        att_output = att_output.permute(1, 0, 2)  # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x.squeeze(-1), att_weight\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"../input/riiid-test-answer-prediction/split10/train_0.pickle\")\n",
    "df = df[df.content_type_id == False]\n",
    "\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "np.random.seed(0)\n",
    "for _, w_df in df.groupby(\"user_id\"):\n",
    "    if np.random.random() < 0.1:\n",
    "        # all val\n",
    "        val_idx.extend(w_df.index.tolist())\n",
    "    else:\n",
    "        train_num = int(len(w_df) * 0.9)\n",
    "        train_idx.extend(w_df[:train_num].index.tolist())\n",
    "        val_idx.extend(w_df[train_num:].index.tolist())\n",
    "\n",
    "df[\"is_val\"] = 0\n",
    "df[\"is_val\"].loc[val_idx] = 1\n",
    "\n",
    "group = df[['user_id', 'content_id', 'answered_correctly', 'is_val']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values,\n",
    "            r[\"is_val\"].values))\n",
    "\n",
    "dataset_train = SAKTDataset(group, 13523)\n",
    "dataset_val = SAKTDataset(group, 13523, is_test=True)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1024, shuffle=True, num_workers=1)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1024, shuffle=False, num_workers=1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = SAKTModel(13523, embed_dim=128)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0.005)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "def train_epoch(model, train_iterator, optim, criterion, device=\"cuda\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    tbar = tqdm(train_iterator)\n",
    "    for item in tbar:\n",
    "        x = item[0].to(device).long()\n",
    "        target_id = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        output, atten_weight = model(x, target_id)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        output = output[:, -1]\n",
    "        label = label[:, -1]\n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "\n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "        tbar.set_description('loss - {:.4f}'.format(loss))\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss, acc, auc = train_epoch(model, dataloader_train, optimizer, criterion, device)\n",
    "    print(\"epoch - {} train_loss - {:.2f} acc - {:.3f} auc - {:.4f}\".format(epoch, loss, acc, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1783/1783 [02:16<00:00, 13.08it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "for d in tqdm(dataloader_val):\n",
    "    x = d[0].to(device).long()\n",
    "    target_id = d[1].to(device).long()\n",
    "    label = d[2].to(device).long()\n",
    "    \n",
    "    output, atten_weight = model(x, target_id)\n",
    "    \n",
    "    preds.extend(torch.nn.Sigmoid()(output[:, -1]).view(-1).data.cpu().numpy().tolist())\n",
    "    labels.extend(label[:, -1].view(-1).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6814781319825192"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6814781319825192"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df.iloc[val_idx][\"answered_correctly\"].values, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_oof = pd.DataFrame()\n",
    "df_oof[\"row_id\"] = df.loc[val_idx].index\n",
    "df_oof[\"predict\"] = preds\n",
    "df_oof[\"target\"] = df.loc[val_idx][\"answered_correctly\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6822792295122452"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_oof[\"target\"].values, df_oof[\"predict\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_oof.to_csv(\"transformers1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof2 = pd.read_csv(\"../output/ex_172/20201202080625/oof_train_1_lgbm.csv\")\n",
    "df_oof2.columns = [\"row_id\", \"predict_lgbm\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_oof2 = pd.merge(df_oof, df_oof2, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909067186967411"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_oof2[\"target\"].values, df_oof2[\"predict_lgbm\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074526256515147"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_oof2[\"target\"].values, df_oof2[\"predict_lgbm\"].values*0.9 + df_oof2[\"predict\"].values*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6831381395586142"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_oof2[\"target\"].values, df_oof2[\"predict\"].values*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
